{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "s-CJYmo1CQCf",
        "outputId": "dd1de582-22f7-43e8-9513-005b5422f812"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f9932b85-3322-4a0d-96ca-43bceec46ca1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f9932b85-3322-4a0d-96ca-43bceec46ca1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Birth_Death_2021-11-09_10-00-00.csv to Birth_Death_2021-11-09_10-00-00.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Birth_Death_2021-11-09_10-00-00.csv': b'\"year\",\"nationality_group_en\",\"nationality_group_ar\",\"gender_en\",\"gender_ar\",\"birth_count\",\"death_count\"\\n\"2019\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"3624\",\"267\"\\n\"2019\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"3474\",\"202\"\\n\"2019\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"12827\",\"1708\"\\n\"2019\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"11941\",\"551\"\\n\"2018\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"3762\",\"244\"\\n\"2018\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"3652\",\"210\"\\n\"2018\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"12403\",\"1595\"\\n\"2018\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"11785\",\"550\"\\n\"2017\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"3958\",\"304\"\\n\"2017\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"3810\",\"239\"\\n\"2017\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"12393\",\"1561\"\\n\"2017\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"11654\",\"498\"\\n\"2016\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"4167\",\"351\"\\n\"2016\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"3980\",\"256\"\\n\"2016\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"12373\",\"1568\"\\n\"2016\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"11367\",\"472\"\\n\"2015\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"4161\",\"343\"\\n\"2015\",\"Emirati\",\"\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"3870\",\"249\"\\n\"2015\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Males\",\"\\xd8\\xb0\\xd9\\x83\\xd9\\x88\\xd8\\xb1\",\"11555\",\"1487\"\\n\"2015\",\"Non Emirati\",\"\\xd8\\xba\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\",\"Females\",\"\\xd8\\xa5\\xd9\\x86\\xd8\\xa7\\xd8\\xab\",\"10788\",\"475\"\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "yLUboLtCCrtN",
        "outputId": "3d3ac9ee-7836-4bdf-e13f-5caa65f2f5d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.83)] [\u001b[0m\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,907 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,660 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,523 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,653 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,606 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,230 kB]\n",
            "Fetched 20.0 MB in 2s (10.3 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ccc069ba790>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ead5be43182c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, MinMaxScaler, VectorAssembler\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"BirthDeathPrediction\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Birth_Death_2021-11-09_10-00-00.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(df) for col in [\"nationality_group_en\", \"gender_en\"]]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Define features and target variable\n",
        "features = [\"year\", \"nationality_group_en_index\", \"gender_en_index\"]\n",
        "target = \"birth_count\"\n",
        "\n",
        "# Normalize numerical features using MinMaxScaler\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features_vec\")\n",
        "df = assembler.transform(df)\n",
        "scaler = MinMaxScaler(inputCol=\"features_vec\", outputCol=\"scaled_features\")\n",
        "df = scaler.fit(df).transform(df)\n",
        "\n",
        "# Prepare input (X) and target (y) variables\n",
        "df = df.select(col(\"scaled_features\").alias(\"features\"), col(target).alias(\"label\"))\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Bagging Ensemble Implementation\n",
        "num_models = 10  # Number of decision trees in the ensemble\n",
        "models = []\n",
        "\n",
        "# Train multiple decision trees on random subsets of data (bootstrap sampling)\n",
        "for _ in range(num_models):\n",
        "    sample_df = train_df.sample(withReplacement=True, fraction=1.0)\n",
        "\n",
        "    # Train a Decision Tree Regressor\n",
        "    dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
        "    model = dt.fit(sample_df)\n",
        "\n",
        "    # Store trained model\n",
        "    models.append(model)\n",
        "\n",
        "# Make predictions using the ensemble (average of individual models)\n",
        "predictions = [model.transform(test_df).select(col(\"prediction\").alias(f\"prediction_{i}\")) for i, model in enumerate(models)]\n",
        "predictions_df = test_df\n",
        "for i, pred in enumerate(predictions):\n",
        "    predictions_df = predictions_df.join(pred, how=\"inner\")\n",
        "predictions_df = predictions_df.withColumn(\"avg_prediction\", sum(col(f\"prediction_{i}\") for i in range(num_models)) / num_models)\n",
        "\n",
        "# Display actual vs. predicted birth counts\n",
        "print(\"Actual vs Predicted Birth Counts:\")\n",
        "predictions_df.select(\"label\", \"avg_prediction\").show()\n",
        "\n",
        "# 10-Fold Cross-Validation Implementation\n",
        "def cross_validate(df, num_folds=10):\n",
        "    fold_size = df.count() // num_folds\n",
        "    errors = []\n",
        "\n",
        "    for i in range(num_folds):\n",
        "        # Define validation and training sets\n",
        "        start, end = i * fold_size, (i + 1) * fold_size\n",
        "        val_df = df.rdd.zipWithIndex().filter(lambda x: start <= x[1] < end).map(lambda x: x[0]).toDF()\n",
        "        train_df = df.rdd.zipWithIndex().filter(lambda x: x[1] < start or x[1] >= end).map(lambda x: x[0]).toDF()\n",
        "\n",
        "        # Train new ensemble model\n",
        "        models = []\n",
        "        for _ in range(num_models):\n",
        "            sample_df = train_df.sample(withReplacement=True, fraction=1.0)\n",
        "            dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
        "            model = dt.fit(sample_df)\n",
        "            models.append(model)\n",
        "\n",
        "        # Predict and compute error (Mean Absolute Error)\n",
        "        predictions = [model.transform(val_df).select(col(\"prediction\").alias(f\"prediction_{i}\")) for i, model in enumerate(models)]\n",
        "        predictions_df = val_df\n",
        "        for i, pred in enumerate(predictions):\n",
        "            predictions_df = predictions_df.join(pred, how=\"inner\")\n",
        "        predictions_df = predictions_df.withColumn(\"avg_prediction\", sum(col(f\"prediction_{i}\") for i in range(num_models)) / num_models)\n",
        "\n",
        "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"avg_prediction\", metricName=\"mae\")\n",
        "        error = evaluator.evaluate(predictions_df)\n",
        "        errors.append(error)\n",
        "\n",
        "    return np.mean(errors)\n",
        "\n",
        "# Perform 10-fold cross-validation and print the mean absolute error\n",
        "cv_error = cross_validate(df, num_folds=10)\n",
        "print(f\"\\nMean Absolute Error from 10-Fold Cross-Validation: {cv_error:.2f}\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "proCUIaeC5DZ",
        "outputId": "95cde05d-6ee2-4fc0-9b2f-ae88d93c2d24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual vs Predicted Birth Counts:\n",
            "+-----+--------------+\n",
            "|label|avg_prediction|\n",
            "+-----+--------------+\n",
            "|10788|       11696.2|\n",
            "|10788|       11696.2|\n",
            "|10788|       10917.8|\n",
            "|10788|       10878.2|\n",
            "|10788|       11771.1|\n",
            "|10788|       11696.2|\n",
            "|10788|       11696.2|\n",
            "|10788|       10917.8|\n",
            "|10788|       10878.2|\n",
            "|10788|       11771.1|\n",
            "|10788|       10926.6|\n",
            "|10788|       10926.6|\n",
            "|10788|       10148.2|\n",
            "|10788|       10108.6|\n",
            "|10788|       11001.5|\n",
            "|10788|       10896.0|\n",
            "|10788|       10896.0|\n",
            "|10788|       10117.6|\n",
            "|10788|       10078.0|\n",
            "|10788|       10970.9|\n",
            "+-----+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Mean Absolute Error from 10-Fold Cross-Validation: 316.70\n"
          ]
        }
      ]
    }
  ]
}